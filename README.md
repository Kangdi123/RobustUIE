# RobustUIE

This is the official code release of the following paper:

Towards Robust Universal Information Extraction: Dataset, Evaluation, and Solution

## Usage

### Environment Setup

```
conda create -n RobustUIE python=3.10

activate RobustUIE

pip install -r requirements.txt
```

### Dataset Preparation

#### 1. Training Data

Note: Because some datasets have copyright requirements and need licenses, we cannot directly release this part of the data now. If you have a license for restricted datasets, you can use them to contact emails in Contact to obtain data.

We conduct experiments utilizing 7 datasets, comprising 3 datasets for the NER task, 2 datasets for the RE task, and 2 datasets for the ED tasks. 

#### 2. RUIE-Bench

We evaluate the robustness of the IE models using our constructed RUIE-Bench. You can download the data from [Google Drive](https://drive.google.com/file/d/1l8oUDkhXjZkW4fnQ2X0kd-MryTEWR-_t).

### Base Model

We use KnowCoder-7b-base as base model, click [here](https://huggingface.co/golaxy/KnowCoder-7B-base) for download.

### Training
You can run it as follows and you can modify the hyper parametrers in scripts/run.sh

```
bash scripts/run.sh
```

When the training is over, you need to merge the lora parameters to the original model.

```
bash scripts/merge_lora_ckpts.sh
```

### Loss-guided Data Augmentation

After training a model, you can run the following script to select augmented samples for data augmentation training of the model.

```
python LDA/LDA_strategy.py
```

### Evaluation
After the training is finished and the model is converted to hf format, you can follow the following steps to inference.

```
bash scripts/inference.sh
```

When the inference result is obtained, it can be tested according to the inference result.

```
bash scripts/eval.sh
```

### Citation

If you find the paper helpful, please cite our work:

```
@article{zhu2025towards,
  title={Towards Robust Universal Information Extraction: Benchmark, Evaluation, and Solution},
  author={Zhu, Jizhao and Shi, Akang and Li, Zixuan and Bai, Long and Jin, Xiaolong and Guo, Jiafeng and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2503.03201},
  year={2025}
}

